{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras.layers import Conv2DTranspose, Conv2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "from IPython.display import display_png\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/eddie/tensorflow/tf/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/eddie/tensorflow/tf/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/eddie/tensorflow/tf/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/eddie/tensorflow/tf/lib/python3.5/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 32)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 56, 56, 128)       295040    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 112, 112, 64)      73792     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 224, 224, 32)      18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 224, 224, 2)       66        \n",
      "=================================================================\n",
      "Total params: 775,202\n",
      "Trainable params: 775,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 8:1:1의 비율로 학습:검증:평가 데이터를 나눔\n",
    "\n",
    "data_path = 'path to image'\n",
    "data_lists = glob.glob(os.path.join(data_path, '*.jpeg'))\n",
    "\n",
    "#math.floor 를 소수점 버림\n",
    "val_n_sample = math.floor(len(data_lists)*0.1)\n",
    "test_n_sample = math.floor(len(data_lists)*0.1)\n",
    "train_n_sample = len(data_lists)-val_n_sample-test_n_sample\n",
    "\n",
    "val_lists = data_lists[:val_n_sample]\n",
    "test_lists = data_lists[val_n_sample:val_n_sample+test_n_sample]\n",
    "train_lists = data_lists[val_n_sample+test_n_sample:train_n_sample+val_n_sample+test_n_sample]\n",
    "\n",
    "#전처리 단계\n",
    "# RGB->LAB로 변환\n",
    "\n",
    "img_size = 224\n",
    "\n",
    "def rgb2lab(rgb):\n",
    "    assert rgb.dtype == 'uint8'\n",
    "    return cv2.cvtColor(rgb,cv2.COLOR_RGB2LAB)\n",
    "\n",
    "def lab2rgb(lab):\n",
    "    assert lab.dtype == 'uint8'\n",
    "    return cv2.cvtColor(lab,cv2.COLOR_LAB2RGB)\n",
    "\n",
    "def get_lab_from_data_list(data_list):\n",
    "    x_lab = []\n",
    "    \n",
    "    for filename in data_list:\n",
    "        rgb = img_to_array(load_img(path=filename, target_size=(img_size,img_size))).astype(np.uint8)\n",
    "        lab = rgb2lab(rgb)\n",
    "        x_lab.append(lab)\n",
    "    \n",
    "    #np.stack을 호출하면 새로운 축방향으로 요소들을 추가\n",
    "    return np.stack(x_lab)\n",
    "\n",
    "#모델 구축\n",
    "\n",
    "autoencoder = Sequential()\n",
    "\n",
    "#encoder\n",
    "autoencoder.add(Conv2D(filters=32,\n",
    "                      kernel_size=(3,3),\n",
    "                      strides=(1,1),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      input_shape=(224,224,1)))\n",
    "autoencoder.add(Conv2D(filters=64,\n",
    "                      kernel_size=(3,3),\n",
    "                      strides=(2,2),\n",
    "                      activation='relu',\n",
    "                      padding='same'))\n",
    "autoencoder.add(Conv2D(filters=128,\n",
    "                      kernel_size=(3,3),\n",
    "                      strides=(2,2),\n",
    "                      activation='relu',\n",
    "                      padding='same'))\n",
    "autoencoder.add(Conv2D(filters=256,\n",
    "                      kernel_size=(3,3),\n",
    "                      strides=(2,2),\n",
    "                      activation='relu',\n",
    "                      padding='same'))\n",
    "\n",
    "#decoder\n",
    "autoencoder.add(Conv2DTranspose(filters=128,\n",
    "                               kernel_size=(3,3),\n",
    "                               strides=(2,2),\n",
    "                               activation='relu',\n",
    "                               padding='same'))\n",
    "autoencoder.add(Conv2DTranspose(filters=64,\n",
    "                               kernel_size=(3,3),\n",
    "                               strides=(2,2),\n",
    "                               activation='relu',\n",
    "                               padding='same'))\n",
    "autoencoder.add(Conv2DTranspose(filters=32,\n",
    "                               kernel_size=(3,3),\n",
    "                               strides=(2,2),\n",
    "                               activation='relu',\n",
    "                               padding='same'))\n",
    "autoencoder.add(Conv2DTranspose(filters=2,\n",
    "                               kernel_size=(1,1),\n",
    "                               strides=(1,1),\n",
    "                               activation='relu',\n",
    "                               padding='same'))\n",
    "#마지막 레이어의 출력 채널이 2개인 이유는 A,B를 생성해야 하기 때문\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델의 학습, 예측\n",
    "#우선 학습, 검증, 평가 각각을 수행하기 위해 제네레이터를 준비\n",
    "#파일들의 경로를 인수로 batch 크기만큼 불러들인 다음, [L], [A,B]를 생성함\n",
    "\n",
    "def generator_with_preprocessing(data_list, batch_size, shuffle=False):\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(data_list)\n",
    "        \n",
    "        for i in range(0, len(data_list), batch_size):\n",
    "            batch_list = data_list[i:i+batch_size]\n",
    "            batch_lab = get_lab_from_data_list(batch_list)\n",
    "            batch_l = batch_lab[:,:,:,0:1]\n",
    "            batch_ab = batch_lab[:,:,:,1:]\n",
    "            yield (batch_l, batch_ab)\n",
    "            \n",
    "batch_size = 'number of batches'\n",
    "\n",
    "train_gen = generator_with_preprocessing(train_lists, batch_size, shuffle=True)\n",
    "val_gen = generator_with_preprocessing(val_lists, batch_size)\n",
    "test_gen = generator_with_preprocessing(test_lists, batch_size)\n",
    "\n",
    "train_steps = math.ceil(len(train_lists)/batch_size)\n",
    "val_steps = math.ceil(len(val_lists)/batch_size)\n",
    "test_steps = math.ceil(len(test_lists)/batch_size)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "autoencoder.fit_generator(generator=train_gen,\n",
    "                         steps_per_epoch=train_steps,\n",
    "                         epochs=epochs,\n",
    "                         validation_data=val_gen,\n",
    "                         validation_steps=val_steps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
